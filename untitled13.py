# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j4SmmNakHp0nYMzvdvZuig9Z9gytxeN4
"""

pip install tensorflow scikit-learn imbalanced-learn pandas numpy keras-tuner scipy

# Advanced Cardiovascular Disease Prediction with State-of-the-Art Techniques
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import PowerTransformer, StandardScaler
from sklearn.metrics import (classification_report, confusion_matrix,
                           roc_auc_score, average_precision_score, precision_recall_curve)
from sklearn.utils.class_weight import compute_class_weight
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import (Dense, Dropout, BatchNormalization,
                                   Input, concatenate, LeakyReLU, Activation,
                                   GaussianNoise)
from tensorflow.keras import regularizers
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau,
                                      ModelCheckpoint)
# Import both SMOTE and ADASYN for robust handling
from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler
import shap
import matplotlib.pyplot as plt
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.calibration import CalibratedClassifierCV

# 1. Advanced Data Cleaning with Clinical Domain Knowledge
def advanced_clinical_cleaning(df):
    """Apply medically-informed data cleaning for cardiovascular data"""
    # Clinical range validation
    df = df[(df['ap_hi'] > 50) & (df['ap_hi'] < 250)]  # Systolic BP range
    df = df[(df['ap_lo'] > 30) & (df['ap_lo'] < 200)]  # Diastolic BP range
    df = df[(df['height'] >= 120) & (df['height'] <= 220)]  # Height range
    df = df[(df['weight'] >= 30) & (df['weight'] <= 250)]  # Weight range

    # Fix BP measurement errors with vector operations
    df['ap_hi'], df['ap_lo'] = np.where(
        df['ap_hi'] < df['ap_lo'],
        (df['ap_lo'], df['ap_hi']),
        (df['ap_hi'], df['ap_lo'])
    )

    # Handle outliers with clinical knowledge
    for col in ['ap_hi', 'ap_lo', 'height', 'weight']:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 2.5 * IQR  # Less strict for medical data
        upper_bound = Q3 + 2.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]

    return df

# 2. State-of-the-Art Feature Engineering
def create_advanced_features(df):
    """Generate medically relevant features based on cardiovascular research"""
    # Age conversion to years
    df['age_years'] = df['age'] / 365.25

    # BMI and body composition metrics
    df['bmi'] = df['weight'] / (df['height']/100)**2
    df['body_surface_area'] = 0.007184 * (df['height']**0.725) * (df['weight']**0.425)  # DuBois formula
    df['obesity_class'] = pd.cut(df['bmi'], bins=[0, 18.5, 25, 30, 35, 100], labels=[0, 1, 2, 3, 4])

    # Advanced cardiovascular metrics
    df['pulse_pressure'] = df['ap_hi'] - df['ap_lo']
    df['mean_arterial_pressure'] = df['ap_lo'] + (df['pulse_pressure']/3)
    df['rate_pressure_product'] = df['ap_hi'] * 75  # Estimate using average heart rate

    # Hypertension classification (based on latest guidelines)
    conditions = [
        (df['ap_hi'] < 120) & (df['ap_lo'] < 80),
        (df['ap_hi'] < 130) & (df['ap_lo'] < 80),
        ((df['ap_hi'] >= 130) & (df['ap_hi'] < 140)) | ((df['ap_lo'] >= 80) & (df['ap_lo'] < 90)),
        (df['ap_hi'] >= 140) | (df['ap_lo'] >= 90)
    ]
    choices = [0, 1, 2, 3]  # Normal, Elevated, Stage1, Stage2
    df['hypertension_stage'] = np.select(conditions, choices, default=3)

    # Risk factor combinations and interactions
    df['lipid_glucose_interaction'] = df['cholesterol'] * df['gluc']
    df['combined_risk_factors'] = ((df['cholesterol'] > 1).astype(int) +
                                  (df['gluc'] > 1).astype(int) +
                                  (df['smoke'] > 0).astype(int) +
                                  (df['alco'] > 0).astype(int) +
                                  (df['active'] < 1).astype(int))

    # Age-related risk modifiers
    df['age_bp_interaction'] = df['age_years'] * df['ap_hi'] / 100
    df['framingham_age_factor'] = np.where(df['age_years'] > 50, df['age_years']/30, df['age_years']/40)

    # Metabolic syndrome risk score
    df['metabolic_risk'] = ((df['bmi'] >= 30).astype(int) +
                           (df['cholesterol'] > 1).astype(int) +
                           (df['gluc'] > 1).astype(int))

    # Lifestyle impact score
    df['lifestyle_score'] = df['active'] - 0.5*(df['smoke'] + df['alco'])

    # Gender-specific risk modulation
    df['gender_risk'] = np.where(df['gender'] == 1,
                                df['age_years']/50,
                                df['age_years']/60)

    return df

# 3. Advanced Neural Network Architecture
def create_deep_cardio_model(input_shape, dropout_rate=0.4):
    """Create a sophisticated neural network for cardiovascular prediction"""
    inputs = Input(shape=(input_shape,))

    # Add noise layer for better generalization
    x = GaussianNoise(0.01)(inputs)

    # First densely connected block
    x = Dense(256)(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = Dropout(dropout_rate)(x)

    # Second block with residual connection
    block_2 = Dense(256)(x)
    block_2 = BatchNormalization()(block_2)
    block_2 = LeakyReLU(alpha=0.1)(block_2)
    block_2 = Dropout(dropout_rate)(block_2)

    # Residual connection
    x = concatenate([x, block_2])

    # Third densely connected block
    x = Dense(128, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = Dropout(dropout_rate)(x)

    # Final prediction layer
    outputs = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# 4. Robust training method with error handling for imbalanced data
def train_robust_model(X_train, y_train, X_val, y_val):
    """Train model with advanced techniques for handling imbalanced medical data"""
    # Compute class weights
    classes = np.array([0, 1])
    class_weights = compute_class_weight('balanced', classes=classes, y=y_train)
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}

    # Model configuration with advanced settings
    model = create_deep_cardio_model(X_train.shape[1])

    # Use Adam optimizer with warmup
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=0.001,
        decay_steps=1000,
        decay_rate=0.9)

    optimizer = Adam(learning_rate=lr_schedule)

    model.compile(
        optimizer=optimizer,
        loss='binary_crossentropy',  # Standard loss function
        metrics=[
            tf.keras.metrics.AUC(name='auc'),
            tf.keras.metrics.Precision(name='precision'),
            tf.keras.metrics.Recall(name='recall'),
            tf.keras.metrics.AUC(curve='PR', name='pr_auc'),
        ]
    )

    # Advanced callbacks
    callbacks = [
        EarlyStopping(monitor='val_pr_auc', patience=25, mode='max', verbose=1,
                    restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10,
                         verbose=1, min_lr=1e-6),
        ModelCheckpoint('best_cardio_model.h5', save_best_only=True,
                      monitor='val_pr_auc', mode='max', verbose=1)
    ]

    # Model training
    history = model.fit(
        X_train, y_train,
        epochs=150,
        batch_size=256,
        validation_data=(X_val, y_val),
        class_weight=class_weight_dict,
        callbacks=callbacks,
        verbose=1
    )

    return model, history

# 5. Advanced threshold optimization
def optimize_threshold_for_clinical_use(model, X_val, y_val, beta=2.0):
    """Find optimal threshold balancing precision and recall for clinical settings
       beta > 1 prioritizes recall (sensitivity) which is important in medical contexts
    """
    # Get model predictions
    y_pred_proba = model.predict(X_val)

    # Calculate precision and recall at various thresholds
    precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)

    # Calculate F-beta scores (beta=2 gives more weight to recall)
    f_beta_scores = ((1 + beta**2) * precision * recall) / ((beta**2 * precision) + recall + 1e-7)

    # Find optimal threshold
    optimal_idx = np.nanargmax(f_beta_scores)

    # Handle edge case
    if optimal_idx < len(thresholds):
        optimal_threshold = thresholds[optimal_idx]
    else:
        optimal_threshold = 0.5

    return optimal_threshold

# Main execution
if __name__ == "__main__":
    # Load data
    print("Loading cardiovascular dataset...")
    df = pd.read_csv('/content/cardio_train.csv', sep=';')

    # Apply advanced data cleaning and feature engineering
    print("Applying clinical data cleaning...")
    df = advanced_clinical_cleaning(df)

    print("Generating advanced cardiovascular features...")
    df = create_advanced_features(df)

    # Feature selection based on medical domain knowledge
    features = [
        # Basic demographics
        'age_years', 'gender', 'height', 'weight', 'bmi',

        # Advanced cardiovascular metrics
        'pulse_pressure', 'mean_arterial_pressure', 'rate_pressure_product',

        # Clinical classifications
        'hypertension_stage', 'obesity_class', 'metabolic_risk',

        # Risk factors and their interactions
        'lipid_glucose_interaction', 'combined_risk_factors', 'age_bp_interaction',
        'framingham_age_factor', 'gender_risk',

        # Lifestyle factors
        'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'lifestyle_score',

        # Advanced physiological metrics
        'body_surface_area'
    ]

    # Extract features and target
    print("Preparing dataset for modeling...")
    X = df[features]
    y = df['cardio']

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.15, stratify=y, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(
        X_train, y_train, test_size=0.15, stratify=y_train, random_state=42)

    # Handle class imbalance with fallback mechanisms
    print("Handling class imbalance...")
    try:
        # Try SMOTE first as it's more robust than ADASYN
        smote = SMOTE(random_state=42)
        X_train, y_train = smote.fit_resample(X_train, y_train)
        print("SMOTE resampling successful.")
    except Exception as e:
        print(f"SMOTE error: {e}. Trying RandomOverSampler as fallback...")
        ros = RandomOverSampler(random_state=42)
        X_train, y_train = ros.fit_resample(X_train, y_train)
        print("RandomOverSampler applied successfully.")

    # Data preprocessing using advanced techniques
    print("Applying advanced data preprocessing...")
    # Using Yeo-Johnson power transformation for robust scaling
    power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)
    X_train_scaled = power_transformer.fit_transform(X_train)
    X_val_scaled = power_transformer.transform(X_val)
    X_test_scaled = power_transformer.transform(X_test)

    # Train neural network
    print("Training advanced neural network model...")
    model, history = train_robust_model(X_train_scaled, y_train, X_val_scaled, y_val)

    # Train XGBoost for ensemble (complementary method)
    print("Training XGBoost model for ensemble...")
    xgb_model = XGBClassifier(
        learning_rate=0.01,
        n_estimators=200,
        max_depth=4,
        subsample=0.8,
        colsample_bytree=0.8,
        objective='binary:logistic',
        scale_pos_weight=1,  # Adjust for class imbalance
        random_state=42
    )
    xgb_model.fit(X_train_scaled, y_train)

    # Create a simple ensemble (average predictions)
    def ensemble_predict(X_data):
        nn_preds = model.predict(X_data)
        xgb_preds = xgb_model.predict_proba(X_data)[:, 1].reshape(-1, 1)
        return (nn_preds + xgb_preds) / 2.0

    # Optimize threshold based on clinical requirements
    print("Optimizing prediction threshold for clinical use...")
    optimal_threshold = optimize_threshold_for_clinical_use(model, X_val_scaled, y_val, beta=2.0)

    # Final evaluation with ensemble model
    print("Evaluating model performance...")
    ensemble_test_preds = ensemble_predict(X_test_scaled)
    y_pred_binary = (ensemble_test_preds >= optimal_threshold).astype(int)

    # Calculate comprehensive metrics
    roc_auc = roc_auc_score(y_test, ensemble_test_preds)
    pr_auc = average_precision_score(y_test, ensemble_test_preds)

    # Calculate confusion matrix and derived metrics
    cm = confusion_matrix(y_test, y_pred_binary)
    tn, fp, fn, tp = cm.ravel()
    sensitivity = tp / (tp + fn)  # Recall/Sensitivity
    specificity = tn / (tn + fp)  # Specificity
    ppv = tp / (tp + fp)  # Positive Predictive Value
    npv = tn / (tn + fn)  # Negative Predictive Value

    # Print detailed performance metrics
    print("\n==== Cardiovascular Disease Prediction Model Results ====")
    print(f"Optimal clinical threshold: {optimal_threshold:.3f}")
    print(f"ROC-AUC: {roc_auc:.4f}")
    print(f"PR-AUC: {pr_auc:.4f}")
    print(f"Sensitivity/Recall: {sensitivity:.4f}")
    print(f"Specificity: {specificity:.4f}")
    print(f"Positive Predictive Value: {ppv:.4f}")
    print(f"Negative Predictive Value: {npv:.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred_binary))
    print("\nConfusion Matrix:")
    print(cm)

    # Generate SHAP values for explainability
    print("\nGenerating model explanations with SHAP...")
    explainer = shap.KernelExplainer(model.predict,
                                     shap.sample(X_train_scaled, 100, random_state=42))
    shap_values = explainer.shap_values(X_test_scaled[:100])

    # Print top features by impact
    feature_importance = np.abs(shap_values).mean(0)
    feature_importance_dict = dict(zip(features, feature_importance))
    sorted_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)

    print("\nTop 10 features by importance:")
    for feature, importance in sorted_features[:10]:
        print(f"{feature}: {importance:.4f}")

    print("\nCardiovascular disease prediction model completed successfully!")